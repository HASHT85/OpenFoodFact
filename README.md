# Atelier IntÃ©gration des DonnÃ©es - OpenFoodFacts ETL

**M1 EISI / M1 CDPIA / M1 CYBER**
**Module:** TRDE703 Atelier IntÃ©gration des DonnÃ©es

## ğŸ“‹ Description du Projet

Projet ETL Big Data qui construit un datamart "OpenFoodFacts Nutrition & QualitÃ©" en utilisant **Apache Spark** (PySpark) pour l'extraction, la transformation et le chargement de donnÃ©es massives vers un datawarehouse **MySQL**.

Le projet implÃ©mente une architecture mÃ©daillon (Bronze â†’ Silver â†’ Gold) avec gestion de la qualitÃ© des donnÃ©es, modÃ©lisation en Ã©toile (star schema), et SCD Type 2 pour l'historisation des produits.

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ OpenFoodFacts   â”‚  Source: JSONL/CSV (donnÃ©es massives)
â”‚   Data Export   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  BRONZE LAYER   â”‚  Ingestion brute avec schÃ©ma explicite
â”‚   (Parquet)     â”‚  Job: etl/jobs/ingest.py
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  SILVER LAYER   â”‚  Nettoyage, normalisation, qualitÃ©
â”‚   (Parquet)     â”‚  Job: etl/jobs/conform.py
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   GOLD LAYER    â”‚  ModÃ¨le en Ã©toile (Star Schema)
â”‚  MySQL Datamart â”‚  Jobs: load_dimensions.py, load_product_scd.py, load_fact.py
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Quality Report â”‚  MÃ©triques & anomalies
â”‚   & Analytics   â”‚  Job: quality_report.py + SQL queries
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ModÃ¨le de DonnÃ©es (Star Schema)

**Dimensions:**
- `dim_brand` - Marques
- `dim_category` - CatÃ©gories de produits
- `dim_country` - Pays
- `dim_time` - Dimension temporelle (YYYYMMDD)
- `dim_product` - Produits (SCD Type 2)

**Faits:**
- `fact_nutrition_snapshot` - Mesures nutritionnelles (100g) avec scores qualitÃ©

Voir `docs/architecture.md` pour les dÃ©tails complets.

## ğŸ“‚ Structure du Projet

```
OpenFoodFact/
â”œâ”€â”€ etl/                        # Code source ETL (PySpark)
â”‚   â”œâ”€â”€ main.py                # Orchestrateur principal
â”‚   â”œâ”€â”€ settings.py            # Configuration
â”‚   â”œâ”€â”€ utils.py               # Utilitaires Spark
â”‚   â”œâ”€â”€ schema_bronze.py       # SchÃ©mas explicites
â”‚   â””â”€â”€ jobs/                  # Jobs ETL (6 fichiers)
â”œâ”€â”€ sql/                        # Scripts SQL
â”‚   â”œâ”€â”€ schema.sql             # DDL tables
â”‚   â”œâ”€â”€ init_dimensions.sql    # Init dimensions
â”‚   â””â”€â”€ analysis_queries.sql   # RequÃªtes analytiques
â”œâ”€â”€ tests/                      # Tests unitaires
â”‚   â”œâ”€â”€ test_etl.py
â”‚   â””â”€â”€ sample_data.jsonl
â”œâ”€â”€ docs/                       # Documentation
â”‚   â”œâ”€â”€ architecture.md
â”‚   â”œâ”€â”€ CAHIER_DE_QUALITE.md
â”‚   â””â”€â”€ DATA_DICTIONARY.md
â”œâ”€â”€ conf/                       # Configuration
â”‚   â””â”€â”€ config.yaml
â”œâ”€â”€ scripts/                    # Scripts utilitaires
â”‚   â””â”€â”€ docker_init.sh
â”œâ”€â”€ data/                       # Data Lake (gÃ©nÃ©rÃ©)
â”œâ”€â”€ docker-compose.yml         # Services Docker
â”œâ”€â”€ Dockerfile                 # Image ETL
â”œâ”€â”€ Makefile                   # Commandes simplifiÃ©es
â””â”€â”€ README.md

Voir PROJECT_STRUCTURE.md pour plus de dÃ©tails.
```

## ğŸš€ Installation & Configuration

### Option 1: Docker (RecommandÃ©) ğŸ³

**La solution la plus simple et reproductible!**

#### PrÃ©requis
- Docker >= 20.10
- Docker Compose >= 1.29
- Make (optionnel mais recommandÃ©)

#### Installation Rapide (3 commandes)

```bash
# 1. Cloner le dÃ©pÃ´t
git clone <repo_url>
cd OpenFoodFact

# 2. Construire et dÃ©marrer les services
make build && make up

# 3. ExÃ©cuter l'ETL avec donnÃ©es de test
make etl-test
```

**C'est tout!** MySQL, PySpark, et toutes les dÃ©pendances sont configurÃ©s automatiquement.

#### Commandes Utiles

```bash
# Voir toutes les commandes disponibles
make help

# Gestion des services
make up              # DÃ©marrer les services
make down            # ArrÃªter les services
make logs            # Voir les logs
make ps              # Ã‰tat des services

# ExÃ©cution ETL
make etl-test        # DonnÃ©es de test
make etl-full        # Dataset complet (aprÃ¨s make download)
make etl-skip        # RÃ©utiliser Bronze existant

# DÃ©veloppement
make shell           # Shell dans conteneur ETL
make mysql-shell     # Console MySQL
make test            # Tests unitaires
```

**ğŸ“– Voir QUICKSTART.md pour dÃ©marrer rapidement**

---

### Option 2: Installation Manuelle

#### PrÃ©requis

- **Python 3.10+** avec pip
- **Java 11 ou 17** (pour PySpark)
- **MySQL 8.0+**
- **Git**

#### Installation

1. **Cloner le dÃ©pÃ´t**
   ```bash
   git clone <repo_url>
   cd OpenFoodFact
   ```

2. **Installer les dÃ©pendances Python**
   ```bash
   pip install -r requirements.txt
   ```

3. **Configurer Java (vÃ©rifier)**
   ```bash
   java -version
   # Doit afficher Java 11 ou 17
   ```

4. **Configurer MySQL**
   ```bash
   # CrÃ©er la base de donnÃ©es
   mysql -u root -p < sql/schema.sql
   mysql -u root -p off_datamart < sql/init_dimensions.sql
   ```

5. **TÃ©lÃ©charger les donnÃ©es OpenFoodFacts** (optionnel)
   ```bash
   python download_dump.py
   # TÃ©lÃ©charge ~5GB de donnÃ©es compressÃ©es
   ```

#### Configuration

Variables d'environnement:

```bash
export DB_HOST=localhost
export DB_PORT=3306
export DB_NAME=off_datamart
export DB_USER=root
export DB_PASSWORD=password
```

## ğŸ’» Utilisation

### Option 1: Pipeline Complet (RecommandÃ©)

```bash
# Avec donnÃ©es de test
python -m etl.main tests/sample_data.jsonl

# Avec donnÃ©es complÃ¨tes OpenFoodFacts
python -m etl.main data/openfoodfacts-products.jsonl

# Skip ingestion (utiliser donnÃ©es Bronze existantes)
python -m etl.main --skip-ingest
```

### Option 2: Jobs Individuels

```bash
# 1. Bronze: Ingestion
python -m etl.jobs.ingest tests/sample_data.jsonl

# 2. Silver: Conformation
python -m etl.jobs.conform

# 3. Gold: Charger dimensions
python -m etl.jobs.load_dimensions

# 4. Gold: Charger produits (SCD2)
python -m etl.jobs.load_product_scd

# 5. Gold: Charger faits
python -m etl.jobs.load_fact

# 6. GÃ©nÃ©rer rapport qualitÃ©
python -m etl.jobs.quality_report
```

### Option 3: Jupyter Notebook (Exploration Interactive)

```bash
jupyter notebook
# Ouvrir: projet/OpenFoodFacts_ETL_Workshop.ipynb
```

## ğŸ“Š RequÃªtes Analytiques

AprÃ¨s chargement du datamart, exÃ©cuter les requÃªtes dans `sql/analysis_queries.sql`:

```sql
-- Top 10 marques par proportion Nutri-Score A/B
SELECT ...

-- Distribution Nutri-Score par catÃ©gorie
SELECT ...

-- Heatmap pays Ã— catÃ©gorie : moyenne sucres
SELECT ...

-- Taux de complÃ©tude par marque
SELECT ...

-- Anomalies dÃ©tectÃ©es
SELECT ...

-- Ã‰volution hebdomadaire complÃ©tude
SELECT ...
```

Voir le fichier complet pour toutes les requÃªtes disponibles.

## ğŸ§ª Tests

```bash
# Lancer tous les tests
pytest tests/test_etl.py -v

# Tests spÃ©cifiques
pytest tests/test_etl.py::TestUtils -v
pytest tests/test_etl.py::TestQualityRules -v

# Avec coverage
pytest tests/test_etl.py --cov=etl --cov-report=html
```

## ğŸ“ˆ QualitÃ© des DonnÃ©es

Le pipeline implÃ©mente plusieurs rÃ¨gles de qualitÃ©:

### RÃ¨gles de Nettoyage (Silver)
- âœ… Normalisation des tags (suppression prÃ©fixes langue)
- âœ… Conversion unitÃ©s (sel = sodium Ã— 2.5)
- âœ… DÃ©doublonnage par code-barres
- âœ… RÃ©solution noms produits (prioritÃ©: fr > en > fallback)

### RÃ¨gles de Validation
- âœ… **Bornes:** Nutriments dans intervalles raisonnables (ex: 0 â‰¤ sugars_100g â‰¤ 100)
- âœ… **ComplÃ©tude:** Score pondÃ©rÃ© de prÃ©sence des champs clÃ©s
- âœ… **CohÃ©rence:** DÃ©tection incohÃ©rences (Ã©nergie nÃ©gative, etc.)

### MÃ©triques Suivies
- Taux de complÃ©tude par champ
- Distribution des scores qualitÃ©
- Nombre d'anomalies par type
- Ã‰volution temporelle de la qualitÃ©

Voir `docs/CAHIER_DE_QUALITE.md` pour les dÃ©tails complets.

## ğŸ”„ SCD Type 2 (Slowly Changing Dimensions)

Les produits sont historisÃ©s avec SCD Type 2:

```sql
SELECT * FROM dim_product WHERE code = '3017620422003';
```

| product_sk | code | product_name | is_current | effective_from | effective_to |
|------------|------|-------------|-----------|---------------|--------------|
| 1 | 3017620422003 | Nutella | 0 | 2023-01-01 | 2023-06-15 |
| 234 | 3017620422003 | Nutella Nouvelle Recette | 1 | 2023-06-15 | NULL |

## ğŸ“ Livrables du Projet

- âœ… **Repo Git structurÃ©** avec code source complet
- âœ… **Pipeline Spark reproductible** (Bronze â†’ Silver â†’ Gold)
- âœ… **Datamart MySQL** avec modÃ¨le en Ã©toile
- âœ… **Scripts DDL/DML** pour crÃ©ation et analyse
- âœ… **Cahier de qualitÃ©** avec rÃ¨gles et mÃ©triques
- âœ… **RequÃªtes analytiques** rÃ©pondant aux KPI mÃ©tiers
- âœ… **Note d'architecture** avec choix techniques
- âœ… **Tests unitaires** pour validation
- âœ… **Documentation complÃ¨te** (README, guides)

## ğŸ¯ KPI & Questions MÃ©tiers

Le datamart rÃ©pond aux questions suivantes:

1. âœ… RÃ©partition Nutri-Score par catÃ©gorie / marque / pays
2. âœ… Ã‰volution complÃ©tude des nutriments dans le temps
3. âœ… Taux d'anomalies (valeurs hors bornes)
4. âœ… Classement marques par qualitÃ© nutritionnelle moyenne
5. âœ… Top catÃ©gories avec le plus de transformation (NOVA)
6. âœ… Heatmap nutritionnelle pays Ã— catÃ©gorie
7. âœ… Produits nÃ©cessitant amÃ©lioration des donnÃ©es

## ğŸ› ï¸ Technologies UtilisÃ©es

- **PySpark 3.5** - Traitement distribuÃ© Big Data
- **MySQL 8.0** - Data Warehouse relationnel
- **Python 3.10** - Langage principal
- **Parquet** - Format stockage Data Lake
- **Docker** - Conteneurisation services
- **Jupyter** - Exploration interactive
- **pytest** - Framework de tests

## ğŸ“– Documentation ComplÃ¨te

- ğŸ“„ [Architecture dÃ©taillÃ©e](docs/architecture.md)
- ğŸ“„ [Cahier de qualitÃ©](docs/CAHIER_DE_QUALITE.md)
- ğŸ“„ [Dictionnaire de donnÃ©es](docs/DATA_DICTIONARY.md)
- ğŸ“„ [RequÃªtes analytiques](sql/analysis_queries.sql)

## ğŸ› DÃ©pannage

### Erreur Java not found
```bash
# Installer Java 17
sudo apt install openjdk-17-jre-headless  # Linux
brew install openjdk@17                     # macOS

# Configurer JAVA_HOME
export JAVA_HOME=/usr/lib/jvm/java-17-openjdk-amd64
```

### Erreur MySQL connection refused
```bash
# VÃ©rifier que MySQL est dÃ©marrÃ©
docker-compose ps
docker-compose up -d mysql

# Tester connexion
mysql -h localhost -u root -p -e "SELECT 1"
```

### Erreur mÃ©moire Spark
```bash
# Augmenter mÃ©moire driver
export SPARK_DRIVER_MEMORY=4g
python -m etl.main <input_file>
```

## ğŸ‘¥ Auteurs

**Ã‰quipe M1 EISI/CDPIA/CYBER**
AnnÃ©e universitaire 2024-2025

## ğŸ“œ Licence

Projet acadÃ©mique - M1 Data Science & AI

## ğŸ”— Ressources Externes

- [OpenFoodFacts](https://world.openfoodfacts.org) - Source des donnÃ©es
- [PySpark Documentation](https://spark.apache.org/docs/latest/api/python/)
- [MySQL 8.0 Reference](https://dev.mysql.com/doc/refman/8.0/en/)

---

**Note:** Ce projet a Ã©tÃ© rÃ©alisÃ© dans le cadre du module TRDE703 "Atelier IntÃ©gration des DonnÃ©es" avec utilisation autorisÃ©e de ChatGPT/Claude comme assistant.
