{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Atelier Intégration des Données - OpenFoodFacts ETL\n",
    "\n",
    "**M1 EISI / CDPIA / CYBER - Module TRDE703**\n",
    "\n",
    "Ce notebook implémente la chaîne ETL complète (Bronze → Silver → Gold) avec **PySpark**.\n",
    "\n",
    "## Architecture\n",
    "\n",
    "```\n",
    "Bronze (Raw JSON/JSONL)\n",
    "    ↓\n",
    "Silver (Cleaned Parquet)\n",
    "    ↓\n",
    "Gold (MySQL Star Schema)\n",
    "    ↓\n",
    "Analytics & Reporting\n",
    "```\n",
    "\n",
    "## Objectifs\n",
    "\n",
    "1. ✅ Ingérer données massives OpenFoodFacts\n",
    "2. ✅ Nettoyer et normaliser (Silver)\n",
    "3. ✅ Charger dans datamart MySQL (Gold)\n",
    "4. ✅ Analyser qualité des données\n",
    "5. ✅ Exécuter requêtes analytiques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configuration & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports standards\n",
    "import sys\n",
    "import os\n",
    "import logging\n",
    "from pathlib import Path\n",
    "\n",
    "# PySpark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import Window\n",
    "\n",
    "# Ajout du module etl au path\n",
    "project_root = Path(os.getcwd()).parent\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "# Import module ETL\n",
    "from etl.utils import setup_logger, create_spark_session\n",
    "from etl.settings import BRONZE_PATH, SILVER_PATH, JDBC_URL, JDBC_PROPERTIES\n",
    "from etl.schema_bronze import get_bronze_schema\n",
    "\n",
    "# Configuration logging\n",
    "logger = setup_logger(\"OFF_Notebook\", logging.INFO)\n",
    "logger.info(\"✓ Imports completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création Spark Session\n",
    "spark = create_spark_session(\"OFF_Workshop_Notebook\")\n",
    "\n",
    "print(f\"Spark Version: {spark.version}\")\n",
    "print(f\"Spark UI: http://localhost:4040\")\n",
    "logger.info(\"✓ Spark Session created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration Paths & Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chemins (relatifs au projet)\n",
    "base_dir = project_root\n",
    "raw_input_path = str(base_dir / \"tests\" / \"sample_data.jsonl\")\n",
    "bronze_path = str(base_dir / \"data\" / \"bronze\")\n",
    "silver_path = str(base_dir / \"data\" / \"silver\")\n",
    "\n",
    "# Configuration MySQL\n",
    "db_config = {\n",
    "    \"host\": \"localhost\",\n",
    "    \"port\": 3306,\n",
    "    \"database\": \"off_datamart\",\n",
    "    \"user\": \"root\",\n",
    "    \"password\": \"password\"  # ⚠️ Changez-moi en production !\n",
    "}\n",
    "\n",
    "jdbc_url = f\"jdbc:mysql://{db_config['host']}:{db_config['port']}/{db_config['database']}\"\n",
    "db_props = {\n",
    "    \"user\": db_config[\"user\"],\n",
    "    \"password\": db_config[\"password\"],\n",
    "    \"driver\": \"com.mysql.cj.jdbc.Driver\"\n",
    "}\n",
    "\n",
    "print(f\"Input file: {raw_input_path}\")\n",
    "print(f\"Bronze path: {bronze_path}\")\n",
    "print(f\"Silver path: {silver_path}\")\n",
    "print(f\"MySQL: {db_config['host']}:{db_config['port']}/{db_config['database']}\")\n",
    "\n",
    "# Vérifier que le fichier d'entrée existe\n",
    "if not os.path.exists(raw_input_path):\n",
    "    print(f\"⚠️ WARNING: Input file not found: {raw_input_path}\")\n",
    "else:\n",
    "    print(f\"✓ Input file found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# BRONZE LAYER - Ingestion Raw Data\n",
    "\n",
    "**Objectif:** Lire les données brutes JSON/JSONL avec un schéma explicite (pas d'inférence)\n",
    "\n",
    "**Format sortie:** Parquet (compression Snappy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Définir le schéma Bronze\n",
    "schema = get_bronze_schema()\n",
    "\n",
    "print(\"Bronze Schema:\")\n",
    "for field in schema.fields:\n",
    "    print(f\"  - {field.name}: {field.dataType}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ingestion avec schéma explicite\n",
    "logger.info(\"Reading raw data...\")\n",
    "\n",
    "df_raw = spark.read.schema(schema).json(raw_input_path)\n",
    "\n",
    "# Métriques\n",
    "total_records = df_raw.count()\n",
    "valid_records = df_raw.filter(F.col(\"code\").isNotNull()).count()\n",
    "invalid_records = total_records - valid_records\n",
    "\n",
    "print(f\"\\n=== BRONZE INGESTION ===\")\n",
    "print(f\"Total records read: {total_records}\")\n",
    "print(f\"Valid records (with code): {valid_records}\")\n",
    "print(f\"Invalid records (no code): {invalid_records}\")\n",
    "print(f\"========================\\n\")\n",
    "\n",
    "# Aperçu des données\n",
    "df_raw.show(3, truncate=False)\n",
    "df_raw.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtrer et écrire Bronze\n",
    "df_bronze = df_raw.filter(F.col(\"code\").isNotNull())\n",
    "\n",
    "logger.info(f\"Writing Bronze layer to: {bronze_path}\")\n",
    "df_bronze.write.mode(\"overwrite\").parquet(bronze_path)\n",
    "\n",
    "print(f\"✓ Bronze layer written: {valid_records} records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# SILVER LAYER - Cleaning & Normalization\n",
    "\n",
    "**Transformations:**\n",
    "1. Résolution multilingue (product_name)\n",
    "2. Aplatissement structures (nutriments)\n",
    "3. Normalisation tags (suppression préfixes langue)\n",
    "4. Conversion unités (sel = sodium × 2.5)\n",
    "5. Dédoublonnage (par code, keep latest)\n",
    "6. Validation qualité (bornes, complétude)\n",
    "7. Hash pour SCD2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lire Bronze\n",
    "logger.info(\"Reading Bronze layer...\")\n",
    "df_bronze = spark.read.parquet(bronze_path)\n",
    "\n",
    "initial_count = df_bronze.count()\n",
    "print(f\"Bronze records: {initial_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Dédoublonnage (garder plus récent par code)\n",
    "logger.info(\"Deduplicating by code...\")\n",
    "\n",
    "window_spec = Window.partitionBy(\"code\").orderBy(F.col(\"last_modified_t\").desc())\n",
    "df_dedup = (\n",
    "    df_bronze\n",
    "    .withColumn(\"_rn\", F.row_number().over(window_spec))\n",
    "    .filter(F.col(\"_rn\") == 1)\n",
    "    .drop(\"_rn\")\n",
    ")\n",
    "\n",
    "dedup_count = df_dedup.count()\n",
    "duplicates = initial_count - dedup_count\n",
    "\n",
    "print(f\"After deduplication: {dedup_count} records ({duplicates} duplicates removed)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Normalisation tags (suppression préfixes en:, fr:, etc.)\n",
    "logger.info(\"Normalizing tags...\")\n",
    "\n",
    "df_normalized = (\n",
    "    df_dedup\n",
    "    .withColumn(\n",
    "        \"countries_normalized\",\n",
    "        F.expr(\"transform(countries_tags, x -> regexp_replace(x, '^[a-z][a-z]:', ''))\")\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"categories_normalized\",\n",
    "        F.expr(\"transform(categories_tags, x -> regexp_replace(x, '^[a-z][a-z]:', ''))\")\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"primary_category\",\n",
    "        F.when(F.size(\"categories_normalized\") > 0, F.col(\"categories_normalized\")[0]).otherwise(None)\n",
    "    )\n",
    ")\n",
    "\n",
    "# Exemple de normalisation\n",
    "print(\"\\nExemple de normalisation:\")\n",
    "df_normalized.select(\n",
    "    \"code\",\n",
    "    \"categories_tags\",\n",
    "    \"categories_normalized\",\n",
    "    \"primary_category\"\n",
    ").show(2, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Aplatissement nutriments & Conversion sel\n",
    "logger.info(\"Flattening nutriments and converting salt...\")\n",
    "\n",
    "df_flat = (\n",
    "    df_normalized\n",
    "    .withColumn(\"energy_kcal_100g\", F.col(\"nutriments.energy-kcal_100g\"))\n",
    "    .withColumn(\"sugars_100g\", F.col(\"nutriments.sugars_100g\"))\n",
    "    .withColumn(\"fat_100g\", F.col(\"nutriments.fat_100g\"))\n",
    "    .withColumn(\"saturated_fat_100g\", F.col(\"nutriments.saturated-fat_100g\"))\n",
    "    .withColumn(\"proteins_100g\", F.col(\"nutriments.proteins_100g\"))\n",
    "    .withColumn(\"fiber_100g\", F.col(\"nutriments.fiber_100g\"))\n",
    "    .withColumn(\"sodium_100g\", F.col(\"nutriments.sodium_100g\"))\n",
    "    # Calcul sel depuis sodium si manquant\n",
    "    .withColumn(\n",
    "        \"salt_100g\",\n",
    "        F.coalesce(F.col(\"nutriments.salt_100g\"), F.col(\"sodium_100g\") * 2.5)\n",
    "    )\n",
    "    .drop(\"nutriments\")\n",
    ")\n",
    "\n",
    "print(\"✓ Nutriments flattened and salt computed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Validation de bornes (détection anomalies)\n",
    "logger.info(\"Checking bounds and detecting anomalies...\")\n",
    "\n",
    "# Flagging out-of-bounds values\n",
    "df_quality = (\n",
    "    df_flat\n",
    "    .withColumn(\n",
    "        \"sugars_100g_out_of_bounds\",\n",
    "        (F.col(\"sugars_100g\").isNotNull()) & ((F.col(\"sugars_100g\") < 0) | (F.col(\"sugars_100g\") > 100))\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"salt_100g_out_of_bounds\",\n",
    "        (F.col(\"salt_100g\").isNotNull()) & ((F.col(\"salt_100g\") < 0) | (F.col(\"salt_100g\") > 25))\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"energy_kcal_100g_out_of_bounds\",\n",
    "        (F.col(\"energy_kcal_100g\").isNotNull()) & ((F.col(\"energy_kcal_100g\") < 0) | (F.col(\"energy_kcal_100g\") > 900))\n",
    "    )\n",
    ")\n",
    "\n",
    "# Compter anomalies\n",
    "anomalies_sugars = df_quality.filter(F.col(\"sugars_100g_out_of_bounds\") == True).count()\n",
    "anomalies_salt = df_quality.filter(F.col(\"salt_100g_out_of_bounds\") == True).count()\n",
    "anomalies_energy = df_quality.filter(F.col(\"energy_kcal_100g_out_of_bounds\") == True).count()\n",
    "\n",
    "print(f\"\\nAnomalies détectées:\")\n",
    "print(f\"  Sugars out of bounds: {anomalies_sugars}\")\n",
    "print(f\"  Salt out of bounds: {anomalies_salt}\")\n",
    "print(f\"  Energy out of bounds: {anomalies_energy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Score de complétude\n",
    "logger.info(\"Computing completeness score...\")\n",
    "\n",
    "# Poids des champs (doivent sommer à 1.0)\n",
    "weights = {\n",
    "    \"product_name\": 0.2,\n",
    "    \"brands\": 0.15,\n",
    "    \"primary_category\": 0.15,\n",
    "    \"nutriscore_grade\": 0.1,\n",
    "    \"energy_kcal_100g\": 0.1,\n",
    "    \"sugars_100g\": 0.075,\n",
    "    \"fat_100g\": 0.075,\n",
    "    \"saturated_fat_100g\": 0.05,\n",
    "    \"salt_100g\": 0.05,\n",
    "    \"proteins_100g\": 0.075,\n",
    "    \"fiber_100g\": 0.05\n",
    "}\n",
    "\n",
    "# Calcul score\n",
    "score_expr = sum(\n",
    "    F.when(F.col(col).isNotNull(), F.lit(weight)).otherwise(F.lit(0.0))\n",
    "    for col, weight in weights.items()\n",
    ")\n",
    "\n",
    "df_complete = df_quality.withColumn(\"completeness_score\", F.round(score_expr, 2))\n",
    "\n",
    "# Statistiques\n",
    "avg_completeness = df_complete.agg(F.avg(\"completeness_score\")).collect()[0][0]\n",
    "print(f\"\\nAverage completeness score: {avg_completeness:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Hash pour SCD2 (détection changements)\n",
    "logger.info(\"Computing row hash for SCD2...\")\n",
    "\n",
    "track_cols = [\"product_name\", \"brands\", \"primary_category\", \"nutriscore_grade\"]\n",
    "concat_expr = F.concat_ws(\n",
    "    \"||\",\n",
    "    *[F.coalesce(F.col(c).cast(\"string\"), F.lit(\"\")) for c in track_cols]\n",
    ")\n",
    "\n",
    "df_silver = df_complete.withColumn(\"row_hash\", F.md5(concat_expr))\n",
    "\n",
    "print(f\"✓ Row hash computed for {df_silver.count()} records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Écrire Silver\n",
    "logger.info(f\"Writing Silver layer to: {silver_path}\")\n",
    "\n",
    "df_silver.write.mode(\"overwrite\").parquet(silver_path)\n",
    "\n",
    "print(f\"✓ Silver layer written: {df_silver.count()} records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aperçu Silver\n",
    "print(\"\\n=== SILVER LAYER SAMPLE ===\")\n",
    "df_silver.select(\n",
    "    \"code\",\n",
    "    \"product_name\",\n",
    "    \"brands\",\n",
    "    \"primary_category\",\n",
    "    \"nutriscore_grade\",\n",
    "    \"sugars_100g\",\n",
    "    \"salt_100g\",\n",
    "    \"completeness_score\"\n",
    ").show(5, truncate=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# GOLD LAYER - MySQL Datamart Loading\n",
    "\n",
    "**Modèle:** Star Schema\n",
    "- Dimensions: brand, category, country, time, product (SCD2)\n",
    "- Fait: nutrition_snapshot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Connexion MySQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test connexion MySQL\n",
    "try:\n",
    "    test_query = \"(SELECT 1 as test) as t\"\n",
    "    df_test = spark.read.jdbc(jdbc_url, test_query, properties=db_props)\n",
    "    result = df_test.collect()[0][0]\n",
    "    print(f\"✓ MySQL connection successful (test query result: {result})\")\nexcept Exception as e:\n",
    "    print(f\"✗ MySQL connection failed: {e}\")\n",
    "    print(\"\\nAssurez-vous que:\")\n",
    "    print(\"  1. MySQL est démarré (docker-compose up -d mysql)\")\n",
    "    print(\"  2. Le schema est créé (mysql < sql/schema.sql)\")\n",
    "    print(\"  3. Les credentials sont corrects\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Charger Dimensions Simples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger dim_brand\n",
    "logger.info(\"Loading dim_brand...\")\n",
    "\n",
    "df_s = spark.read.parquet(silver_path)\n",
    "\n",
    "brands = (\n",
    "    df_s.select(\"brands\")\n",
    "    .filter(F.col(\"brands\").isNotNull())\n",
    "    .distinct()\n",
    "    .withColumnRenamed(\"brands\", \"brand_name\")\n",
    ")\n",
    "\n",
    "new_brand_count = brands.count()\n",
    "\n",
    "try:\n",
    "    existing_brands = spark.read.jdbc(jdbc_url, \"dim_brand\", properties=db_props)\n",
    "    new_brands = brands.join(existing_brands, on=\"brand_name\", how=\"left_anti\")\n",
    "    \n",
    "    insert_count = new_brands.count()\n",
    "    \n",
    "    if insert_count > 0:\n",
    "        new_brands.write.jdbc(jdbc_url, \"dim_brand\", mode=\"append\", properties=db_props)\n",
    "        print(f\"✓ dim_brand: {insert_count} new brands inserted\")\n",
    "    else:\n",
    "        print(\"✓ dim_brand: No new brands to insert\")\n",
    "except Exception as e:\n",
    "    # Première fois: insérer tous\n",
    "    brands.write.jdbc(jdbc_url, \"dim_brand\", mode=\"append\", properties=db_props)\n",
    "    print(f\"✓ dim_brand: {new_brand_count} brands inserted (first load)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger dim_category\n",
    "logger.info(\"Loading dim_category...\")\n",
    "\n",
    "categories = (\n",
    "    df_s.select(F.explode(\"categories_normalized\").alias(\"category_code\"))\n",
    "    .filter(F.col(\"category_code\").isNotNull())\n",
    "    .distinct()\n",
    "    .withColumn(\"category_name_fr\", F.col(\"category_code\"))\n",
    "    .withColumn(\"level\", F.lit(1))\n",
    "    .withColumn(\"parent_category_sk\", F.lit(None).cast(\"int\"))\n",
    ")\n",
    "\n",
    "try:\n",
    "    existing_categories = spark.read.jdbc(jdbc_url, \"dim_category\", properties=db_props)\n",
    "    new_categories = categories.join(existing_categories, on=\"category_code\", how=\"left_anti\")\n",
    "    \n",
    "    insert_count = new_categories.count()\n",
    "    \n",
    "    if insert_count > 0:\n",
    "        new_categories.write.jdbc(jdbc_url, \"dim_category\", mode=\"append\", properties=db_props)\n",
    "        print(f\"✓ dim_category: {insert_count} new categories inserted\")\n",
    "    else:\n",
    "        print(\"✓ dim_category: No new categories to insert\")\n",
    "except Exception as e:\n",
    "    categories.write.jdbc(jdbc_url, \"dim_category\", mode=\"append\", properties=db_props)\n",
    "    print(f\"✓ dim_category: {categories.count()} categories inserted (first load)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Charger Produits (SCD Type 2)\n",
    "\n",
    "**SCD2:** Historisation des changements de métadonnées produit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Préparer données produits\n",
    "logger.info(\"Preparing product data...\")\n",
    "\n",
    "# Récupérer mappings brand_sk et category_sk\n",
    "brand_mapping = spark.read.jdbc(jdbc_url, \"dim_brand\", properties=db_props)\n",
    "category_mapping = spark.read.jdbc(jdbc_url, \"dim_category\", properties=db_props)\n",
    "\n",
    "df_product_input = (\n",
    "    df_s\n",
    "    .join(brand_mapping, df_s.brands == brand_mapping.brand_name, \"left\")\n",
    "    .drop(\"brand_name\")\n",
    "    .join(category_mapping, df_s.primary_category == category_mapping.category_code, \"left\")\n",
    "    .drop(\"category_code\")\n",
    "    .withColumnRenamed(\"category_sk\", \"primary_category_sk\")\n",
    "    .withColumn(\"countries_multi\", F.to_json(\"countries_normalized\"))\n",
    "    .select(\n",
    "        \"code\",\n",
    "        \"product_name\",\n",
    "        \"brand_sk\",\n",
    "        \"primary_category_sk\",\n",
    "        \"countries_multi\",\n",
    "        \"row_hash\"\n",
    "    )\n",
    ")\n",
    "\n",
    "print(f\"Products prepared: {df_product_input.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger produits avec SCD2\n",
    "logger.info(\"Loading products (SCD2)...\")\n",
    "\n",
    "try:\n",
    "    query = \"(SELECT product_sk, code, row_hash FROM dim_product WHERE is_current = 1) as active\"\n",
    "    df_active = spark.read.jdbc(jdbc_url, query, properties=db_props)\n",
    "    \n",
    "    df_joined = df_product_input.alias(\"inp\").join(\n",
    "        df_active.alias(\"act\"),\n",
    "        F.col(\"inp.code\") == F.col(\"act.code\"),\n",
    "        \"left\"\n",
    "    )\n",
    "    \n",
    "    # Nouveaux produits\n",
    "    df_new = df_joined.filter(F.col(\"act.product_sk\").isNull()).select(\n",
    "        F.col(\"inp.code\"),\n",
    "        F.col(\"inp.product_name\"),\n",
    "        F.col(\"inp.brand_sk\"),\n",
    "        F.col(\"inp.primary_category_sk\"),\n",
    "        F.col(\"inp.countries_multi\"),\n",
    "        F.lit(1).cast(\"boolean\").alias(\"is_current\"),\n",
    "        F.current_timestamp().alias(\"effective_from\"),\n",
    "        F.lit(None).cast(\"timestamp\").alias(\"effective_to\"),\n",
    "        F.col(\"inp.row_hash\")\n",
    "    )\n",
    "    \n",
    "    new_count = df_new.count()\n",
    "    \n",
    "    if new_count > 0:\n",
    "        df_new.write.jdbc(jdbc_url, \"dim_product\", mode=\"append\", properties=db_props)\n",
    "        print(f\"✓ dim_product: {new_count} new products inserted\")\n",
    "    else:\n",
    "        print(\"✓ dim_product: No new products to insert\")\n",
    "        \n",
    "except Exception as e:\n",
    "    # Première fois: insérer tous\n",
    "    df_first_load = df_product_input.select(\n",
    "        \"code\",\n",
    "        \"product_name\",\n",
    "        \"brand_sk\",\n",
    "        \"primary_category_sk\",\n",
    "        \"countries_multi\",\n",
    "        F.lit(1).cast(\"boolean\").alias(\"is_current\"),\n",
    "        F.current_timestamp().alias(\"effective_from\"),\n",
    "        F.lit(None).cast(\"timestamp\").alias(\"effective_to\"),\n",
    "        \"row_hash\"\n",
    "    )\n",
    "    \n",
    "    df_first_load.write.jdbc(jdbc_url, \"dim_product\", mode=\"append\", properties=db_props)\n",
    "    print(f\"✓ dim_product: {df_first_load.count()} products inserted (first load)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Charger Faits (Nutrition Snapshot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Préparer faits avec FK\n",
    "logger.info(\"Preparing fact data...\")\n",
    "\n",
    "product_mapping = spark.read.jdbc(\n",
    "    jdbc_url,\n",
    "    \"(SELECT product_sk, code FROM dim_product WHERE is_current = 1) as t\",\n",
    "    properties=db_props\n",
    ")\n",
    "\n",
    "df_fact = (\n",
    "    df_s\n",
    "    .join(product_mapping, on=\"code\", how=\"inner\")\n",
    "    .withColumn(\"time_sk\", F.from_unixtime(F.col(\"last_modified_t\"), \"yyyyMMdd\").cast(\"int\"))\n",
    "    .select(\n",
    "        \"product_sk\",\n",
    "        \"time_sk\",\n",
    "        \"energy_kcal_100g\",\n",
    "        \"fat_100g\",\n",
    "        \"saturated_fat_100g\",\n",
    "        \"sugars_100g\",\n",
    "        \"salt_100g\",\n",
    "        \"proteins_100g\",\n",
    "        \"fiber_100g\",\n",
    "        \"sodium_100g\",\n",
    "        \"nutriscore_grade\",\n",
    "        \"completeness_score\"\n",
    "    )\n",
    ")\n",
    "\n",
    "fact_count = df_fact.count()\n",
    "print(f\"Facts prepared: {fact_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger faits\n",
    "logger.info(\"Loading fact table...\")\n",
    "\n",
    "df_fact.write.jdbc(jdbc_url, \"fact_nutrition_snapshot\", mode=\"append\", properties=db_props)\n",
    "\n",
    "print(f\"✓ fact_nutrition_snapshot: {fact_count} records inserted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# ANALYSE & REPORTING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rapport de Qualité"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistiques qualité\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"RAPPORT DE QUALITÉ DES DONNÉES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "total = df_s.count()\n",
    "\n",
    "# Complétude\n",
    "avg_completeness = df_s.agg(F.avg(\"completeness_score\")).collect()[0][0]\n",
    "high_quality = df_s.filter(F.col(\"completeness_score\") >= 0.8).count()\n",
    "low_quality = df_s.filter(F.col(\"completeness_score\") < 0.5).count()\n",
    "\n",
    "print(f\"\\nCOMPLÉTUDE:\")\n",
    "print(f\"  Total produits: {total}\")\n",
    "print(f\"  Score moyen: {avg_completeness:.3f}\")\n",
    "print(f\"  Haute qualité (≥0.8): {high_quality} ({high_quality/total*100:.1f}%)\")\n",
    "print(f\"  Faible qualité (<0.5): {low_quality} ({low_quality/total*100:.1f}%)\")\n",
    "\n",
    "# Nutri-Score\n",
    "with_nutriscore = df_s.filter(F.col(\"nutriscore_grade\").isNotNull()).count()\n",
    "print(f\"\\nNUTRI-SCORE:\")\n",
    "print(f\"  Avec Nutri-Score: {with_nutriscore} ({with_nutriscore/total*100:.1f}%)\")\n",
    "\n",
    "nutriscore_dist = df_s.groupBy(\"nutriscore_grade\").count().orderBy(\"nutriscore_grade\").collect()\n",
    "print(f\"  Distribution:\")\n",
    "for row in nutriscore_dist:\n",
    "    grade = row[\"nutriscore_grade\"] if row[\"nutriscore_grade\"] else \"null\"\n",
    "    count = row[\"count\"]\n",
    "    pct = count / total * 100\n",
    "    print(f\"    Grade {grade}: {count:4d} ({pct:5.1f}%)\")\n",
    "\n",
    "# Anomalies\n",
    "print(f\"\\nANOMALIES:\")\n",
    "print(f\"  Sugars out of bounds: {anomalies_sugars}\")\n",
    "print(f\"  Salt out of bounds: {anomalies_salt}\")\n",
    "print(f\"  Energy out of bounds: {anomalies_energy}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requêtes Analytiques SQL\n",
    "\n",
    "Exemples de requêtes métiers sur le datamart MySQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Requête 1: Top marques par Nutri-Score A/B\n",
    "query1 = \"\"\"\n",
    "SELECT\n",
    "    b.brand_name,\n",
    "    COUNT(DISTINCT p.product_sk) AS total_products,\n",
    "    SUM(CASE WHEN f.nutriscore_grade IN ('a', 'b') THEN 1 ELSE 0 END) AS products_ab,\n",
    "    ROUND(\n",
    "        SUM(CASE WHEN f.nutriscore_grade IN ('a', 'b') THEN 1 ELSE 0 END) * 100.0 / COUNT(DISTINCT p.product_sk),\n",
    "        2\n",
    "    ) AS pct_nutriscore_ab\n",
    "FROM fact_nutrition_snapshot f\n",
    "JOIN dim_product p ON f.product_sk = p.product_sk AND p.is_current = 1\n",
    "JOIN dim_brand b ON p.brand_sk = b.brand_sk\n",
    "GROUP BY b.brand_name\n",
    "HAVING COUNT(DISTINCT p.product_sk) >= 1\n",
    "ORDER BY pct_nutriscore_ab DESC\n",
    "LIMIT 10\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\n=== TOP 10 MARQUES PAR NUTRI-SCORE A/B ===\")\n",
    "df_query1 = spark.read.jdbc(jdbc_url, f\"({query1}) as t\", properties=db_props)\n",
    "df_query1.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Requête 2: Taux de complétude par marque\n",
    "query2 = \"\"\"\n",
    "SELECT\n",
    "    b.brand_name,\n",
    "    COUNT(DISTINCT p.product_sk) AS total_products,\n",
    "    ROUND(AVG(f.completeness_score), 3) AS avg_completeness\n",
    "FROM fact_nutrition_snapshot f\n",
    "JOIN dim_product p ON f.product_sk = p.product_sk AND p.is_current = 1\n",
    "JOIN dim_brand b ON p.brand_sk = b.brand_sk\n",
    "GROUP BY b.brand_name\n",
    "ORDER BY avg_completeness DESC\n",
    "LIMIT 10\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\n=== TAUX DE COMPLÉTUDE PAR MARQUE ===\")\n",
    "df_query2 = spark.read.jdbc(jdbc_url, f\"({query2}) as t\", properties=db_props)\n",
    "df_query2.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# FIN DU WORKSHOP\n",
    "\n",
    "## Résumé\n",
    "\n",
    "✅ **Bronze:** Données brutes ingérées avec schéma explicite\n",
    "\n",
    "✅ **Silver:** Nettoyage, normalisation, qualité contrôlée\n",
    "\n",
    "✅ **Gold:** Datamart MySQL (Star Schema) chargé\n",
    "\n",
    "✅ **Analytics:** Requêtes SQL métiers exécutées\n",
    "\n",
    "## Prochaines Étapes\n",
    "\n",
    "1. Tester avec dataset complet: `python download_dump.py`\n",
    "2. Exécuter toutes les requêtes analytiques: `sql/analysis_queries.sql`\n",
    "3. Créer un dashboard (Grafana, Tableau, etc.)\n",
    "4. Implémenter chargement incrémental\n",
    "\n",
    "## Ressources\n",
    "\n",
    "- Documentation: `docs/`\n",
    "- Tests: `pytest tests/test_etl.py`\n",
    "- Pipeline complet: `python -m etl.main <input_file>`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arrêter Spark\n",
    "spark.stop()\n",
    "print(\"✓ Spark session stopped\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
