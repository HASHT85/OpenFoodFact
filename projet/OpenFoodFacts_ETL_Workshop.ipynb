{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Atelier Intégration des Données - OpenFoodFacts\n",
                "\n",
                "Ce notebook implémente la chaîne ETL complète (Bronze -> Silver -> Gold) avec **PySpark**.\n",
                "Il charge les données depuis un fichier JSONL, les nettoie, et les charge dans un Datamart MySQL."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 1. Imports & Configuration\n",
                "import sys\n",
                "import os\n",
                "import logging\n",
                "from pyspark.sql import SparkSession\n",
                "from pyspark.sql import functions as F\n",
                "from pyspark.sql import Window\n",
                "from pyspark.sql.types import StructType, StructField, StringType, LongType, DoubleType, ArrayType, MapType\n",
                "\n",
                "# Configuration Logger\n",
                "logging.basicConfig(level=logging.INFO, format='%(asctime)s %(levelname)s: %(message)s')\n",
                "logger = logging.getLogger(\"OFF_Notebook\")\n",
                "\n",
                "# Spark Session\n",
                "spark = SparkSession.builder \\\n",
                "    .appName(\"OFF_Workshop_Notebook\") \\\n",
                "    .config(\"spark.sql.session.timeZone\", \"UTC\") \\\n",
                "    .config(\"spark.driver.extraClassPath\", \"mysql-connector-j-8.0.33.jar\") \\ \n",
                "    .getOrCreate()\n",
                "\n",
                "logger.info(\"Spark Session Created\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 2. Configuration & Paramètres\n",
                "# A REMPLIR : Informations de connexion MySQL\n",
                "db_props = {\n",
                "    \"user\": \"root\",\n",
                "    \"password\": \"password\",  # Changez moi\n",
                "    \"driver\": \"com.mysql.cj.jdbc.Driver\"\n",
                "}\n",
                "jdbc_url = \"jdbc:mysql://localhost:3306/off_datamart\"\n",
                "\n",
                "# Chemins des données (Utilisation de dossiers relatifs)\n",
                "# On remonte d'un niveau car le notebook est dans le dossier 'projet'\n",
                "base_dir = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
                "raw_input_path = os.path.join(base_dir, \"tests\", \"sample_data.jsonl\")\n",
                "bronze_path = os.path.join(base_dir, \"data\", \"bronze\")\n",
                "silver_path = os.path.join(base_dir, \"data\", \"silver\")\n",
                "\n",
                "logger.info(f\"Input: {raw_input_path}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Bronze Layer (Ingestion)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def get_bronze_schema():\n",
                "    nutriments_schema = StructType([\n",
                "        StructField(\"energy-kcal_100g\", DoubleType(), True),\n",
                "        StructField(\"sugars_100g\", DoubleType(), True),\n",
                "        StructField(\"fat_100g\", DoubleType(), True),\n",
                "        StructField(\"saturated-fat_100g\", DoubleType(), True),\n",
                "        StructField(\"salt_100g\", DoubleType(), True),\n",
                "        StructField(\"sodium_100g\", DoubleType(), True),\n",
                "        StructField(\"proteins_100g\", DoubleType(), True),\n",
                "        StructField(\"fiber_100g\", DoubleType(), True)\n",
                "    ])\n",
                "\n",
                "    schema = StructType([\n",
                "        StructField(\"code\", StringType(), True),\n",
                "        StructField(\"product_name\", StringType(), True),\n",
                "        StructField(\"brands\", StringType(), True),\n",
                "        StructField(\"categories_tags\", ArrayType(StringType()), True),\n",
                "        StructField(\"countries_tags\", ArrayType(StringType()), True),\n",
                "        StructField(\"nutriscore_grade\", StringType(), True),\n",
                "        StructField(\"nutriments\", nutriments_schema, True),\n",
                "        StructField(\"last_modified_t\", LongType(), True)\n",
                "    ])\n",
                "    return schema\n",
                "\n",
                "# Ingestion\n",
                "logger.info(\"Reading Raw Data...\")\n",
                "df_raw = spark.read.schema(get_bronze_schema()).json(raw_input_path)\n",
                "df_raw.write.mode(\"overwrite\").parquet(bronze_path)\n",
                "logger.info(\"Bronze Layer Written.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Silver Layer (Conformation)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Transformations Silver\n",
                "logger.info(\"Processing Silver Layer...\")\n",
                "df_bronze = spark.read.parquet(bronze_path)\n",
                "\n",
                "# 1. Dédoublonnage (Keep latest last_modified_t)\n",
                "window_spec = Window.partitionBy(\"code\").orderBy(F.col(\"last_modified_t\").desc())\n",
                "df_dedup = df_bronze.withColumn(\"rn\", F.row_number().over(window_spec)) \\\n",
                "                    .filter(F.col(\"rn\") == 1) \\\n",
                "                    .drop(\"rn\")\n",
                "\n",
                "# 2. Normalisation (Tags & Units)\n",
                "df_silver = df_dedup.withColumn(\"countries_normalized\", F.expr(\"transform(countries_tags, x -> regexp_replace(x, '^..:', ''))\")) \\\n",
                "                    .withColumn(\"categories_normalized\", F.expr(\"transform(categories_tags, x -> regexp_replace(x, '^..:', ''))\")) \\\n",
                "                    .withColumn(\"nutriments.salt_100g\", F.coalesce(F.col(\"nutriments.salt_100g\"), F.col(\"nutriments.sodium_100g\") * 2.5))\n",
                "\n",
                "df_silver.write.mode(\"overwrite\").parquet(silver_path)\n",
                "logger.info(\"Silver Layer Written.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Gold Layer (Datamart Loading)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Chargement Dimensions Simples\n",
                "logger.info(\"Loading Dimensions...\")\n",
                "df_s = spark.read.parquet(silver_path)\n",
                "\n",
                "# Brand\n",
                "brands = df_s.select(\"brands\").where(F.col(\"brands\").isNotNull()).distinct().withColumnRenamed(\"brands\", \"brand_name\")\n",
                "\n",
                "try:\n",
                "    existing_brands = spark.read.jdbc(jdbc_url, \"dim_brand\", properties=db_props)\n",
                "    new_brands = brands.join(existing_brands, on=\"brand_name\", how=\"left_anti\")\n",
                "    if new_brands.count() > 0:\n",
                "        new_brands.write.jdbc(jdbc_url, \"dim_brand\", mode=\"append\", properties=db_props)\n",
                "    logger.info(\"Dimensions Loaded.\")\n",
                "except Exception as e:\n",
                "    logger.warning(f\"DB Error (Check connection): {e}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Chargement Produit (SCD2)\n",
                "logger.info(\"Loading Products SCD2...\")\n",
                "# Hashing pour détection de changement\n",
                "track_cols = [\"product_name\", \"brands\", \"categories_tags\", \"countries_tags\", \"nutriscore_grade\"]\n",
                "concat_expr = F.concat_ws(\"||\", *[F.coalesce(F.col(c).cast(\"string\"), F.lit(\"\")) for c in track_cols])\n",
                "df_input = df_s.withColumn(\"row_hash\", F.md5(concat_expr))\n",
                "\n",
                "try:\n",
                "    dbtable = \"(SELECT product_sk, code, row_hash as current_hash FROM dim_product WHERE is_current = 1) as t\"\n",
                "    df_active = spark.read.jdbc(jdbc_url, dbtable, properties=db_props)\n",
                "    df_joined = df_input.join(df_active, \"code\", \"left\")\n",
                "    df_new_or_changed = df_joined.filter(F.col(\"product_sk\").isNull() | (F.col(\"row_hash\") != F.col(\"current_hash\")))\n",
                "    \n",
                "    if df_new_or_changed.count() > 0:\n",
                "        df_final = df_new_or_changed.select(\n",
                "            F.col(\"code\"),\n",
                "            F.col(\"product_name\"),\n",
                "            F.lit(1).alias(\"is_current\"),\n",
                "            F.current_timestamp().alias(\"effective_from\"),\n",
                "            F.col(\"row_hash\")\n",
                "        )\n",
                "        df_final.write.jdbc(jdbc_url, \"dim_product\", mode=\"append\", properties=db_props)\n",
                "        logger.info(\"Products Updated.\")\n",
                "    else:\n",
                "        logger.info(\"No product changes.\")\n",
                "except Exception as e:\n",
                "    logger.warning(f\"Product Load Error: {e}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Qualité & Reporting"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "total = df_s.count()\n",
                "with_score = df_s.filter(F.col(\"nutriscore_grade\").isNotNull()).count()\n",
                "completeness = (with_score / total) * 100 if total > 0 else 0\n",
                "\n",
                "print(f\"=== RAPPORT DE QUALITÉ ===\")\n",
                "print(f\"Produits traités : {total}\")\n",
                "print(f\"Complétude Nutri-Score : {completeness:.2f}%\")\n",
                "print(f\"=========================\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}