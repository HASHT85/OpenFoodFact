# Note d'Architecture - OpenFoodFacts ETL

## 1. Vue d'Ensemble

Ce projet implÃ©mente un pipeline ETL Big Data complet pour construire un datamart "OpenFoodFacts Nutrition & QualitÃ©" Ã  partir de donnÃ©es massives (millions de produits alimentaires).

**Objectif:** Transformer des donnÃ©es brutes JSON/JSONL en un datawarehouse structurÃ© (modÃ¨le en Ã©toile) permettant l'analyse nutritionnelle et qualitÃ© des produits alimentaires mondiaux.

---

## 2. Choix Techniques

### 2.1 Stack Technologique

| Composant | Technologie | Version | Justification |
|-----------|-------------|---------|---------------|
| **ETL Engine** | Apache Spark | 3.5.0 | Traitement distribuÃ© pour Big Data (millions de records) |
| **Language** | Python (PySpark) | 3.10+ | Standard data science, API Spark mature |
| **Data Warehouse** | MySQL | 8.0+ | SGBD relationnel robuste, requis par sujet |
| **Data Lake Format** | Apache Parquet | - | Format colonnaire optimisÃ© pour analytics |
| **Configuration** | YAML | - | Configuration externalisÃ©e et lisible |
| **Testing** | pytest | - | Framework standard pour Python |
| **Containerization** | Docker | - | MySQL isolÃ©, reproductibilitÃ© |

### 2.2 Justifications des Choix

**PySpark vs. Pandas:**
- âœ… Spark: DistribuÃ©, scalable Ã  millions/milliards de lignes
- âŒ Pandas: LimitÃ© Ã  mÃ©moire RAM, pas de distribution

**MySQL vs. PostgreSQL:**
- âœ… MySQL: Requis par Ã©noncÃ©, performances excellentes pour OLAP
- âœ… PostgreSQL: Alternative viable (JSON natif, window functions)

**Parquet vs. CSV:**
- âœ… Parquet: Compression ~10x, lecture colonnaire rapide, typage
- âŒ CSV: Texte brut, pas de typage, lent pour analytics

**SCD Type 2 vs. Type 1:**
- âœ… Type 2: Historisation complÃ¨te, audit trail
- âŒ Type 1: Pas d'historique, overwrites

---

## 3. Architecture de DonnÃ©es (Medallion Architecture)

### 3.1 Concept Medallion

Architecture en 3 couches popularisÃ©e par Databricks:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    BRONZE    â”‚  Raw Data (As-Is)
â”‚   (Parquet)  â”‚  - Ingestion sans transformation
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  - SchÃ©ma explicite
       â”‚          - Audit trail complet
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    SILVER    â”‚  Cleaned Data (Conformed)
â”‚   (Parquet)  â”‚  - Nettoyage et normalisation
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  - QualitÃ© contrÃ´lÃ©e
       â”‚          - DÃ©doublonnage
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     GOLD     â”‚  Business-Level Data (Presentation)
â”‚    (MySQL)   â”‚  - ModÃ¨le en Ã©toile
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  - AgrÃ©gations
                  - Analytics-ready
```

### 3.2 Couche Bronze (Raw)

**RÃ´le:** Ingestion brute des donnÃ©es sources sans transformation.

| Aspect | DÃ©tail |
|--------|--------|
| **Source** | Exports JSONL OpenFoodFacts (~5GB compressÃ©, 3M+ produits) |
| **Format stockage** | Parquet (compressÃ© Snappy) |
| **SchÃ©ma** | Explicite (dÃ©fini dans `schema_bronze.py`) |
| **Partitionnement** | Aucun (pour ce workshop) |
| **RÃ©tention** | Permanent (archive) |

**Job:** `etl/jobs/ingest.py`

**Traitements:**
- Lecture JSON/JSONL avec schÃ©ma explicite (pas d'infÃ©rence)
- Filtrage enregistrements sans `code` (clÃ© obligatoire)
- Ã‰criture Parquet avec compression

**MÃ©triques:**
- Nombre d'enregistrements lus
- Nombre d'enregistrements rejetÃ©s (pas de code)
- Taille fichiers Bronze

### 3.3 Couche Silver (Clean & Conformed)

**RÃ´le:** Nettoyage, normalisation, et prÃ©paration pour analyses.

| Aspect | DÃ©tail |
|--------|--------|
| **Format** | Parquet (schÃ©ma Silver optimisÃ©) |
| **QualitÃ©** | RÃ¨gles appliquÃ©es, anomalies flaggÃ©es |
| **DÃ©doublonnage** | Oui (par code, keep latest) |
| **Enrichissement** | Calculs dÃ©rivÃ©s (sel, scores) |

**Job:** `etl/jobs/conform.py`

**Traitements:**

1. **RÃ©solution Multilingue**
   ```python
   product_name = COALESCE(
       product_name_fr,
       product_name_en,
       product_name,
       generic_name,
       'Unknown Product'
   )
   ```

2. **Aplatissement Structures**
   - Nutriments: `nutriments.sugars_100g` â†’ `sugars_100g` (top-level)
   - CatÃ©gories: `categories_tags[]` â†’ `categories_normalized[]` + `primary_category`

3. **Normalisation Tags**
   ```python
   "en:breakfast" â†’ "breakfast"
   "fr:petit-dejeuner" â†’ "petit-dejeuner"
   ```

4. **Conversion UnitÃ©s**
   ```python
   salt_100g = COALESCE(salt_100g, sodium_100g Ã— 2.5)
   ```

5. **DÃ©doublonnage**
   - ClÃ©: `code` (code-barres)
   - StratÃ©gie: Keep record avec `last_modified_t` max
   - FenÃªtrage: `ROW_NUMBER() OVER (PARTITION BY code ORDER BY last_modified_t DESC)`

6. **QualitÃ©**
   - Validation bornes (0 â‰¤ sugars â‰¤ 100)
   - Score complÃ©tude pondÃ©rÃ© (0.00-1.00)
   - Flaggage anomalies (`_out_of_bounds` colonnes)
   - Hash MD5 pour SCD2 (colonnes suivies)

**MÃ©triques:**
- Nombre de doublons supprimÃ©s
- Taux d'anomalies par type
- Score moyen de complÃ©tude
- Distribution Nutri-Score

### 3.4 Couche Gold (Presentation)

**RÃ´le:** ModÃ¨le en Ã©toile pour analytics et reporting.

| Aspect | DÃ©tail |
|--------|--------|
| **Technologie** | MySQL 8.0 (InnoDB) |
| **ModÃ¨le** | Star Schema (5 dimensions + 1 fait) |
| **Historisation** | SCD Type 2 sur `dim_product` |
| **Index** | StratÃ©giques pour requÃªtes frÃ©quentes |

**Jobs:**
- `etl/jobs/load_dimensions.py` - Dimensions simples
- `etl/jobs/load_product_scd.py` - Produits avec SCD2
- `etl/jobs/load_fact.py` - Table de faits

**ModÃ¨le DÃ©taillÃ©:**

```
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚  dim_time   â”‚
                    â”‚  time_sk PK â”‚
                    â”‚  date       â”‚
                    â”‚  year/month â”‚
                    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚                 â”‚                 â”‚
    â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
    â”‚dim_brand â”‚    â”‚ dim_product   â”‚  â”‚dim_countryâ”‚
    â”‚brand_sk  â”‚â—„â”€â”€â”€â”‚  product_sk   â”‚  â”‚country_sk â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚  code (NK)    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚  is_current   â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚  effective_*  â”‚
    â”‚dim_categ â”‚â—„â”€â”€â”€â”‚  row_hash     â”‚
    â”‚categ_sk  â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚
                            â”‚
                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                   â”‚ fact_nutrition_snap  â”‚
                   â”‚   fact_id PK         â”‚
                   â”‚   product_sk FK      â”‚
                   â”‚   time_sk FK         â”‚
                   â”‚   --- Mesures ---    â”‚
                   â”‚   energy_kcal_100g   â”‚
                   â”‚   sugars_100g        â”‚
                   â”‚   fat_100g           â”‚
                   â”‚   salt_100g          â”‚
                   â”‚   proteins_100g      â”‚
                   â”‚   fiber_100g         â”‚
                   â”‚   --- Scores ---     â”‚
                   â”‚   nutriscore_grade   â”‚
                   â”‚   nova_group         â”‚
                   â”‚   --- QualitÃ© ---    â”‚
                   â”‚   completeness_score â”‚
                   â”‚   quality_issues_jsonâ”‚
                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 4. StratÃ©gie d'Alimentation (Upsert & SCD)

### 4.1 Dimensions Simples (Brand, Category, Country)

**StratÃ©gie:** Insert-Ignore (Append-Only pour nouvelles valeurs)

**Algorithme:**
```python
existing = read_from_mysql("dim_brand")
new_values = silver_data.select("brands").distinct()
to_insert = new_values.anti_join(existing, on="brand_name")
to_insert.write_to_mysql("dim_brand", mode="append")
```

**Justification:**
- Pas de mises Ã  jour (noms de marques stables)
- VolumÃ©trie faible (100K marques max)
- Pas d'historisation nÃ©cessaire

### 4.2 Dimension Temporelle (Time)

**StratÃ©gie:** PrÃ©-chargement complet (2020-2030)

**Algorithme:**
```python
dates = generate_date_range("2020-01-01", "2030-12-31")
for date in dates:
    time_sk = date.strftime("%Y%m%d")  # ex: 20240315
    insert into dim_time (time_sk, date, year, month, day, iso_week)
```

**Justification:**
- Dimension fixe (pas de donnÃ©es mÃ©tier)
- Performance (prÃ©-join vs. calcul runtime)
- Pattern standard data warehousing

### 4.3 Dimension Produits (SCD Type 2)

**StratÃ©gie:** Slowly Changing Dimension Type 2

**Colonnes SCD:**
- `is_current` (BOOLEAN) - 1 pour version actuelle, 0 pour historique
- `effective_from` (TIMESTAMP) - Date dÃ©but validitÃ©
- `effective_to` (TIMESTAMP) - Date fin validitÃ© (NULL si actuel)
- `row_hash` (VARCHAR(32)) - MD5 des colonnes suivies

**Colonnes Suivies (pour dÃ©tection changement):**
- `product_name`
- `brands`
- `primary_category`
- `nutriscore_grade`
- `nova_group`
- `ecoscore_grade`

**Algorithme:**

```python
# 1. Lire versions actives
active_products = read_mysql("SELECT product_sk, code, row_hash
                              FROM dim_product WHERE is_current = 1")

# 2. Join entrÃ©es avec actives
joined = input_data.join(active_products, on="code", how="left")

# 3. DÃ©tecter changements
new = joined.filter("product_sk IS NULL")  # Nouveaux produits
changed = joined.filter("product_sk IS NOT NULL AND
                        input_hash != current_hash")  # ChangÃ©s
unchanged = joined.filter("product_sk IS NOT NULL AND
                          input_hash = current_hash")  # InchangÃ©s

# 4. GÃ©rer changements
for changed_product in changed:
    # Expirer ancienne version
    UPDATE dim_product
    SET is_current = 0, effective_to = NOW()
    WHERE code = changed_product.code AND is_current = 1

    # InsÃ©rer nouvelle version
    INSERT INTO dim_product (code, ..., is_current, effective_from)
    VALUES (changed_product.code, ..., 1, NOW())

# 5. InsÃ©rer nouveaux
INSERT INTO dim_product (code, ..., is_current, effective_from)
SELECT ... FROM new
```

**Exemple RÃ©sultat:**

| product_sk | code | product_name | brand_sk | is_current | effective_from | effective_to |
|------------|------|-------------|----------|-----------|---------------|--------------|
| 1 | 301762 | Nutella | 5 | 0 | 2023-01-01 | 2023-06-15 |
| 234 | 301762 | Nutella Nouvelle Recette | 5 | 1 | 2023-06-15 | NULL |

**RequÃªte Type:**
```sql
-- Version actuelle
SELECT * FROM dim_product WHERE code = '301762' AND is_current = 1;

-- Historique complet
SELECT * FROM dim_product WHERE code = '301762' ORDER BY effective_from;

-- Ã‰tat au 2023-05-01
SELECT * FROM dim_product
WHERE code = '301762'
  AND effective_from <= '2023-05-01'
  AND (effective_to > '2023-05-01' OR effective_to IS NULL);
```

### 4.4 Table de Faits (Append-Only)

**StratÃ©gie:** Insertion pure (snapshots)

**Algorithme:**
```python
# 1. RÃ©soudre FK
product_mapping = read_mysql("SELECT product_sk, code FROM dim_product
                              WHERE is_current = 1")
facts = silver.join(product_mapping, on="code")

# 2. Calculer time_sk
facts = facts.withColumn("time_sk",
                        from_unixtime(col("last_modified_t"), "yyyyMMdd").cast("int"))

# 3. InsÃ©rer
facts.write_to_mysql("fact_nutrition_snapshot", mode="append")
```

**Justification:**
- Pas de mises Ã  jour (faits historiques immutables)
- Snapshots par run ETL
- Analyse temporelle via time_sk

---

## 5. Flux de DonnÃ©es Complet

### 5.1 Pipeline Principal

```mermaid
graph TD
    A[OpenFoodFacts JSONL] --> B[Bronze: Ingest]
    B --> C[Silver: Conform]
    C --> D[Gold: Load Dimensions]
    D --> E[Gold: Load Products SCD2]
    E --> F[Gold: Load Facts]
    F --> G[Quality Report]
    G --> H[Analytics SQL]
```

### 5.2 Commande d'ExÃ©cution

```bash
# Pipeline complet (automatique)
python -m etl.main data/openfoodfacts-products.jsonl

# Ou Ã©tape par Ã©tape
python -m etl.jobs.ingest data/input.jsonl
python -m etl.jobs.conform
python -m etl.jobs.load_dimensions
python -m etl.jobs.load_product_scd
python -m etl.jobs.load_fact
python -m etl.jobs.quality_report
```

### 5.3 DurÃ©es EstimÃ©es

| Ã‰tape | 10K produits | 1M produits | 3M produits |
|-------|--------------|-------------|-------------|
| Ingest | 10s | 2min | 5min |
| Conform | 15s | 3min | 8min |
| Load Dims | 5s | 1min | 2min |
| Load Products | 10s | 2min | 5min |
| Load Facts | 20s | 4min | 10min |
| Quality Report | 10s | 2min | 5min |
| **TOTAL** | **1min 10s** | **14min** | **35min** |

*Sur machine standard (8 cores, 16GB RAM, SSD)*

---

## 6. Gestion de la QualitÃ©

Voir `CAHIER_DE_QUALITE.md` pour dÃ©tails complets.

**Points ClÃ©s:**
- âœ… Validation Ã  chaque couche (Bronze/Silver/Gold)
- âœ… StratÃ©gie "Filter & Flag" (pas de rejet silencieux)
- âœ… Score de complÃ©tude pondÃ©rÃ© (0.00-1.00)
- âœ… DÃ©tection anomalies (bornes, cohÃ©rence)
- âœ… Rapports automatiques JSON horodatÃ©s
- âœ… Alertes configurables (seuils)

---

## 7. ScalabilitÃ© et Performance

### 7.1 Optimisations Spark

| Optimisation | Configuration | Impact |
|--------------|--------------|--------|
| **Adaptive Query** | `spark.sql.adaptive.enabled=true` | Auto-optimisation joins |
| **Coalesce Partitions** | `spark.sql.adaptive.coalescePartitions=true` | RÃ©duction shuffle overhead |
| **Broadcast Join** | Automatique pour dim < 10MB | AccÃ©lÃ©ration 10x pour petites dims |
| **Shuffle Partitions** | `spark.sql.shuffle.partitions=200` | Ã‰quilibrage charge |

### 7.2 Optimisations MySQL

| Optimisation | ImplÃ©mentation | Gain |
|--------------|----------------|------|
| **Index PK** | Auto sur product_sk, fact_id | Jointures rapides |
| **Index FK** | Sur toutes FK (product_sk, time_sk) | RequÃªtes 100x plus rapides |
| **Index composites** | (code, is_current), (product_sk, time_sk) | RequÃªtes SCD2 optimisÃ©es |
| **InnoDB Buffer Pool** | 50-70% RAM disponible | Cache en mÃ©moire |

### 7.3 ScalabilitÃ© Horizontale (Production)

Pour scalabilitÃ© au-delÃ  de 100M+ produits:

1. **Spark Cluster:**
   - YARN ou Kubernetes
   - 10+ worker nodes
   - Partitionnement intelligent (par pays, catÃ©gorie)

2. **Data Lake:**
   - S3 ou HDFS (pas filesystem local)
   - Partitionnement Parquet (year/month/day)

3. **Data Warehouse:**
   - MySQL Sharding ou migration vers Snowflake/BigQuery
   - Tables partitionnÃ©es (RANGE sur time_sk)

4. **Orchestration:**
   - Airflow avec DAGs
   - Monitoring (Prometheus, Grafana)

---

## 8. SÃ©curitÃ© et ConformitÃ©

### 8.1 DonnÃ©es Sensibles

**OpenFoodFacts = DonnÃ©es Publiques (Open Data)**
- âœ… Pas de PII (Personally Identifiable Information)
- âœ… Licence ODbL (Open Database License)
- âœ… Pas de GDPR concerns

### 8.2 AccÃ¨s Base de DonnÃ©es

**Production Best Practices:**
- ğŸ”’ Credentials via variables environnement (pas hardcodÃ©s)
- ğŸ”’ Principe moindre privilÃ¨ge (user ETL != user analytics)
- ğŸ”’ SSL/TLS pour connexions MySQL
- ğŸ”’ Audit trail (etl_run_log table)

### 8.3 ConformitÃ©

- âœ… TraÃ§abilitÃ© complÃ¨te (mÃ©tadonnÃ©es runs)
- âœ… ReproductibilitÃ© (code versionnÃ© Git)
- âœ… Documentation exhaustive
- âœ… Tests automatisÃ©s

---

## 9. Ã‰volutions Futures

### Version 1.1 (Court Terme)
- [ ] Chargement incrÃ©mental (CDC - Change Data Capture)
- [ ] Enrichissement via API OFF pour champs manquants
- [ ] Dashboard Grafana temps rÃ©el
- [ ] Alertes email automatiques

### Version 2.0 (Moyen Terme)
- [ ] ML pour dÃ©tection anomalies avancÃ©es
- [ ] RÃ©solution taxonomies hiÃ©rarchiques
- [ ] Bridge tables pour N-N (produits-catÃ©gories)
- [ ] dim_nutri sÃ©parÃ©e (scores nutritionnels)

### Version 3.0 (Long Terme)
- [ ] Migration Spark â†’ Databricks/EMR
- [ ] Data Warehouse â†’ Snowflake/BigQuery
- [ ] Data lineage complet (provenance)
- [ ] Feedback loop vers OpenFoodFacts

---

## 10. RÃ©fÃ©rences Techniques

### Standards AppliquÃ©s
- **Kimball Methodology:** ModÃ©lisation dimensionnelle
- **Medallion Architecture:** Bronze/Silver/Gold layers
- **SCD Type 2:** Historisation lente

### Documentation Externe
- [PySpark API](https://spark.apache.org/docs/latest/api/python/)
- [MySQL 8.0 Reference](https://dev.mysql.com/doc/refman/8.0/en/)
- [OpenFoodFacts Data](https://world.openfoodfacts.org/data)
- [Kimball DW Toolkit](https://www.kimballgroup.com/)

---

**Version:** 1.0.0
**Date:** 2024-01-23
**Auteurs:** Ã‰quipe M1 EISI/CDPIA/CYBER
